{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5976561a-09a3-4174-b60a-a2f2451318a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef103b0c-2276-415f-ad80-d5610b1dcfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gld_sr(f,a,b,tol=1e-6):\n",
    "    r = (np.sqrt(5)-1)/2\n",
    "\n",
    "    i=0\n",
    "    while abs(b-a)>tol:\n",
    "        x1 = a + (1-r)*(b-a)\n",
    "        x2 = a + r*(b-a)\n",
    "        if f(x1)<f(x2):\n",
    "            b=x2\n",
    "        else:\n",
    "            a=x1\n",
    "        print(f\"Iter:{i+1}| interval:{[a,b]}\")\n",
    "        i+=1\n",
    "    x_min = (a+b)/2\n",
    "    return x_min,f(x_min),i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c458da60-b2a2-4999-9275-4f229c5a7cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib(n):\n",
    "    f = [0,1]\n",
    "    for i in range(2,n+1):\n",
    "        f.append(f[i-1]+f[i-2])\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3045bbc-9a4f-41f6-94ce-075e015dec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib_s(f,a,b,n):\n",
    "    F = fib(n)\n",
    "    L0 = (b-a)\n",
    "    for i in range(2,n+2):\n",
    "        L0i = (F[n+1]/F[n])*L0\n",
    "        x1 = a+L0i\n",
    "        x2 = b-L0i\n",
    "        if f(x1)<f(x2):\n",
    "            b=x2\n",
    "        else:\n",
    "            a= x1\n",
    "        print(f\"Iter:{i-1}| interval:{[a,b]}\")\n",
    "    x_min = (a+b)/2\n",
    "    return x_min,f(x_min),i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "770eacd6-f2bc-48c0-b600-c7ff285b1ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(f,x,tol=1e-5):\n",
    "    n = len(x)\n",
    "    x_syms = sp.symbols(f\"x1:{n+1}\")\n",
    "    grad_f = sp.Matrix([sp.diff(f,var) for var in x_syms])\n",
    "    hess_f = sp.Matrix(n,n, lambda i,j: sp.diff(f,x_syms[i],x_syms[j]))\n",
    "    grad_func = sp.lambdify(x_syms,grad_f,'numpy')\n",
    "    hess_func = sp.lambdify(x_syms,hess_f,'numpy')\n",
    "    func = sp.lambdify(x_syms,f,'numpy')\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        grad = np.array(grad_func(*x),dtype=float).flatten()\n",
    "        print(f\"\")\n",
    "        if np.linalg.norm(grad)<=tol:\n",
    "            print(\"Grad-->0\")\n",
    "            break\n",
    "        hess = np.array(hess_func(*x),dtype=float)\n",
    "        try:\n",
    "            d = np.dot(np.linalg.inv(hess),grad)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Hessina is singular\")\n",
    "            break\n",
    "        x = x - d\n",
    "        i+=1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65382a4b-6106-4aee-87ff-dafad386df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_Search(f,grad,x,d,c=1e-4,rho=0.5):\n",
    "    alpha = 1\n",
    "    if f(*(x + alpha*d))>f(*x) + alpha*c*np.dot(grad,d):\n",
    "        alpha *= rho\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb83bc8-82a2-44b0-89de-819ea1878c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steeapest_d(f,x.tol=1e-5):\n",
    "    n = len(x)\n",
    "    x_syms = sp.symbols(f\"x1:{n+1}\")\n",
    "    grad_f = sp.Matrix([sp.diff(f,var) for var in x_syms])\n",
    "    \n",
    "    grad_func = sp.lambdify(x_syms,grad_f,'numpy')\n",
    "    \n",
    "    func = sp.lambdify(x_syms,f,'numpy')\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while True:\n",
    "        grad = np.array(grad_func(*x),dtype=float).flatten()\n",
    "        d = -grad\n",
    "        alpha = line_search(f,grad,x,d)\n",
    "        print(f\"\")\n",
    "        if np.linalg.norm(grad)<=tol:\n",
    "            print(\"Grad-->0\")\n",
    "            break\n",
    "        x = x +alpha*d\n",
    "        i+=1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce9865-dfa6-4da7-bc62-8eb191551c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b571d3b-50eb-4f82-b720-128693b5ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_con_w_d(f,x,d,Q,tol=1e-5):\n",
    "    n = len(x)\n",
    "    x_syms = sp.symbols(f\"x1:{n+1}\")\n",
    "    grad_f = sp.Matrix([sp.diff(f,var) for var in x_syms])\n",
    "    \n",
    "    grad_func = sp.lambdify(x_syms,grad_f,'numpy')\n",
    "    \n",
    "    func = sp.lambdify(x_syms,f,'numpy')\n",
    "\n",
    "    i = 0\n",
    "    for i in range(d.shape[0]):\n",
    "        grad = np.array(grad_func(*x),dtype=float).flatten()\n",
    "        Qd = np.dot(Q,d[i])\n",
    "        alpha = -np.dot(grad.T ,d[i])/np.dot(d[i].T ,Qd)\n",
    "        \n",
    "        if np.linalg.norm(grad)<=tol:\n",
    "            print(\"Grad-->0\")\n",
    "            break\n",
    "        print(f\"\")\n",
    "        x = x + np.dot(alpha,d[i])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4447712d-a05d-43d9-b189-4a3914e4a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_con_wt_dir(f,x,Q,tol=1e-2):\n",
    "    n = len(x)\n",
    "    x_syms = sp.symbols(f\"x1:{n+1}\")\n",
    "    grad_f = sp.Matrix([sp.diff(f,var) for var in x_syms])\n",
    "    \n",
    "    grad_func = sp.lambdify(x_syms,grad_f,'numpy')\n",
    "    \n",
    "    func = sp.lambdify(x_syms,f,'numpy')\n",
    "\n",
    "    i = 0\n",
    "    grad = np.array(grad_func(*x),dtype=float).flatten()\n",
    "    d = -grad\n",
    "    while True:\n",
    "        if np.linalg.norm(grad) <= tol:\n",
    "            print(\"Gradient tends to 0\")\n",
    "            print(f\"||grad|| = {np.linalg.norm(grad):.6f}\")\n",
    "            \n",
    "            break\n",
    "        alpha = -(grad.T @ d)/(d.T @ (Q @ d))\n",
    "        print(f\"Iter {i+1}: alpha = {alpha:.6f}, x = {x}, f(x) = {func(*x):.6f}, ||grad|| = {np.linalg.norm(grad):.6f}, direction : {d}\")\n",
    "        x = x + alpha*d\n",
    "        grad = np.array(grad_func(*x),dtype=float).flatten()\n",
    "        b = (grad.T @(Q@ d))/(d.T @ (Q @ d))\n",
    "        d= -grad + b*d\n",
    "        i+=1\n",
    "    return x,func(*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b08d102-a057-494b-b424-ecc8253b890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: alpha = 1.000000, x = [0. 0.], f(x) = 0.000000, ||grad|| = 1.414214, direction : [-1.  1.]\n",
      "Iter 2: alpha = 0.250000, x = [-1.  1.], f(x) = -1.000000, ||grad|| = 1.414214, direction : [0. 2.]\n",
      "Gradient tends to 0\n",
      "||grad|| = 0.000000\n",
      "Minimum at: [-1.   1.5]\n",
      "Minimum value: -1.25\n"
     ]
    }
   ],
   "source": [
    "x1, x2 = sp.symbols('x1 x2 ')\n",
    "f = x1 - x2 + 2*x1**2 + 2*x1*x2 + x2**2\n",
    "x0 = np.array([0.0, 0.0])\n",
    "Q = np.array([[4, 2], [2, 2]])\n",
    "result = q_con_wt_dir(f, x0, Q)\n",
    "print(\"Minimum at:\", result[0])\n",
    "print(\"Minimum value:\", result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b268000-304f-41d9-aa24-54d7bcc611c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfgs(f,x_vals,tol=1e-3):\n",
    "    n = len(x_vals)\n",
    "    x_syms = sp.symbols(f\"x1:{n+1}\")\n",
    "\n",
    "    grad_f = sp.Matrix([sp.diff(f,var) for var in x_syms])\n",
    "\n",
    "    grad_func = sp.lambdify(x_syms,grad_f,'numpy')\n",
    "    func = sp.lambdify(x_syms,f,'numpy')\n",
    "\n",
    "    B = np.eye(n)\n",
    "    H = np.linalg.inv(B)\n",
    "\n",
    "    grad = np.array(grad_func(*x_vals),dtype = float).flatten()\n",
    "\n",
    "    i=0\n",
    "    while True:\n",
    "        \n",
    "        # if np.linalg.norm(grad) <=tol:\n",
    "        #     break\n",
    "        d = (H @ -grad)\n",
    "        alpha = line_search(func,grad,x_vals,d) #this is backtracking line sear incase exact one does not converge\n",
    "        #alpha = exact_line_search(f, x_syms, x_vals, d)\n",
    "        print(f\"Iter:{i+1}: alpha:{alpha}, x:{x_vals}, f(x):{func(*x_vals)}, grad:{np.linalg.norm(grad)}\")\n",
    "        if np.linalg.norm(grad) <=tol:\n",
    "            break\n",
    "        x_new = x_vals + alpha * d\n",
    "        grad_new = np.array(grad_func(*x_new), dtype=float).flatten()\n",
    "\n",
    "        s = x_new - x_vals\n",
    "        y = grad_new - grad\n",
    "        \n",
    "        s = s.reshape(-1,1)\n",
    "        y = y.reshape(-1,1)\n",
    "        \n",
    "        #Bs = B @ s\n",
    "        #B = B - np.outer(Bs, B.T @ s) / (s.T @ Bs) + np.outer(y, y) / (y.T @ s)\n",
    "\n",
    "        B =  B - ((B @ s) @ (s.T @ B))/(s.T @ (B @ s)) + (y @ y.T)/(y.T @ s) \n",
    "        \n",
    "        try: \n",
    "            H = np.linalg.inv(B)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Hessian Matrix is Singular\")\n",
    "            B = np.eye(n)\n",
    "            H = np.linalg.inv(B)\n",
    "        \n",
    "        x_vals = x_new\n",
    "        grad = grad_new\n",
    "        i+=1\n",
    "    return x_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d480016-586f-4374-bed4-a9d9a2d69979",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = sp.symbols('x1 x2 ')\n",
    "\n",
    "f=2*x1**2 + x2**2 + 2*x1*x2 + x1 - x2\n",
    "x0 = np.array([0.0, 0.0])\n",
    "\n",
    "result = bfgs(f, x0)\n",
    "print(\"Minimum at:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e25ae55b-0246-4666-adac-d18d4fe96189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -1.250000\n",
      "         Iterations: 4\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "\n",
      "Optimization result:\n",
      "  message: Optimization terminated successfully.\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: -1.25\n",
      "        x: [-1.000e+00  1.500e+00]\n",
      "      nit: 4\n",
      "      jac: [ 0.000e+00  0.000e+00]\n",
      " hess_inv: [[ 5.000e-01 -5.000e-01]\n",
      "            [-5.000e-01  1.000e+00]]\n",
      "     nfev: 6\n",
      "     njev: 6\n",
      "\n",
      "Iteration details:\n",
      "Iteration 1: x1 = -0.714178, x2 = 0.714178, f(x) = -0.918306, grad = [-0.4283557 -1.       ]\n",
      "Iteration 2: x1 = -0.789924, x2 = 1.319946, f(x) = -1.204967, grad = [0.48019662 0.06004441]\n",
      "Iteration 3: x1 = -1.012614, x2 = 1.510811, f(x) = -1.249838, grad = [-0.02883312 -0.00360533]\n",
      "Iteration 4: x1 = -1.000000, x2 = 1.500000, f(x) = -1.250000, grad = [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x):\n",
    "    x1, x2 = x\n",
    "    return 2*x1**2 + x2**2 + 2*x1*x2 + x1 - x2\n",
    "\n",
    "# Define the gradient (Jacobian)\n",
    "def gradient(x):\n",
    "    x1, x2 = x\n",
    "    df_dx1 = 4*x1 + 2*x2 + 1\n",
    "    df_dx2 = 2*x2 + 2*x1 - 1\n",
    "    return np.array([df_dx1, df_dx2])\n",
    "\n",
    "# Callback to record iteration details\n",
    "iteration_details = []\n",
    "\n",
    "def callback(xk):\n",
    "    fval = objective_function(xk)\n",
    "    grad = gradient(xk)\n",
    "    iteration_details.append({\n",
    "        'x': xk.copy(),\n",
    "        'fval': fval,\n",
    "        'grad': grad.copy()\n",
    "    })\n",
    "\n",
    "# Initial guess\n",
    "x0 = np.array([0.0, 0.0])\n",
    "\n",
    "# Options for optimizer\n",
    "options = {'disp': True, 'maxiter': 100}\n",
    "\n",
    "# Perform the minimization\n",
    "result = minimize(objective_function, x0, method='BFGS', jac=gradient,\n",
    "                  options=options, callback=callback)\n",
    "\n",
    "# Print final result\n",
    "print(\"\\nOptimization result:\")\n",
    "print(result)\n",
    "\n",
    "# Print iteration details\n",
    "print(\"\\nIteration details:\")\n",
    "for i, details in enumerate(iteration_details):\n",
    "    x1, x2 = details['x']\n",
    "    print(f\"Iteration {i+1}: x1 = {x1:.6f}, x2 = {x2:.6f}, f(x) = {details['fval']:.6f}, grad = {details['grad']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e552ca48-ff02-4c43-a460-736be85c76d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
